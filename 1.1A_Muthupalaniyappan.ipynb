{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pkg_resources\n",
    "import nltk\n",
    "import math\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absl-py==0.10.0', 'argon2-cffi==20.1.0', 'asn1crypto==0.24.0', 'astunparse==1.6.3', 'async-generator==1.10', 'attrs==20.3.0', 'babel==2.7.0', 'backcall==0.1.0', 'bleach==3.1.0', 'blinker==1.4', 'blis==0.4.1', 'boto3==1.10.44', 'botocore==1.13.44', 'bottleneck==1.2.1', 'cachetools==3.1.1', 'catalogue==1.0.0', 'certifi==2020.6.20', 'cffi==1.14.3', 'chardet==3.0.4', 'click==7.1.2', 'cryptography==2.7', 'cycler==0.10.0', 'cymem==2.0.3', 'cython==0.29.21', 'datasets==1.8.0', 'decorator==4.4.2', 'defusedxml==0.6.0', 'dill==0.3.1.dev0', 'docutils==0.15.2', 'en-core-web-sm==2.3.1', 'en-vectors-web-lg==2.3.0', 'entrypoints==0.3', 'filelock==3.0.12', 'flatbuffers==20210625202600', 'fsspec==2021.4.0', 'funcsigs==1.0.2', 'future==0.18.2', 'gast==0.3.3', 'gensim==3.8.3', 'gevent==1.3a2', 'google-api-core==1.14.2', 'google-auth-oauthlib==0.4.1', 'google-auth==1.6.3', 'google-cloud-core==1.0.3', 'google-cloud-storage==1.18.0', 'google-pasta==0.2.0', 'google-resumable-media==0.3.2', 'googleapis-common-protos==1.6.0', 'greenlet==0.4.17', 'grpcio==1.32.0', 'h5py==2.10.0', 'huggingface-hub==0.0.8', 'ibm-cloud-sdk-core==3.10.0', 'ibm-watson==5.1.0', 'idna==2.8', 'ipykernel==5.3.4', 'ipython-genutils==0.2.0', 'ipython==7.18.1', 'ipywidgets==7.5.1', 'jedi==0.13.3', 'jinja2==2.10.3', 'jmespath==0.9.4', 'joblib==0.14.0', 'jsonschema==3.2.0', 'jupyter-client==6.1.7', 'jupyter-core==4.6.3', 'jupyterlab-pygments==0.1.1', 'keras-preprocessing==1.1.2', 'kerberos==1.3.0', 'keyboard==0.13.5', 'kiwisolver==1.1.0', 'markdown==3.1.1', 'markovify==0.8.3', 'markupsafe==1.1.1', 'matplotlib==3.3.3', 'mistune==0.8.4', 'mouseinfo==0.1.3', 'multiprocess==0.70.9', 'murmurhash==1.0.2', 'nbclient==0.5.0', 'nbconvert==6.0.1', 'nbformat==5.0.7', 'nest-asyncio==1.4.0', 'nltk==3.5', 'notebook==6.1.4', 'numexpr==2.7.0', 'numpy==1.19.5', 'oauthlib==3.1.0', 'open3d==0.9.0.0', 'opt-einsum==3.3.0', 'packaging==19.2', 'pandas==1.2.0', 'pandocfilters==1.4.2', 'parso==0.6.1', 'pexpect==4.7.0', 'pickleshare==0.7.5', 'pillow==7.2.0', 'plac==1.1.3', 'preshed==3.0.2', 'prometheus-client==0.7.1', 'prompt-toolkit==2.0.9', 'protobuf==3.12.2', 'psutil==5.7.2', 'ptyprocess==0.6.0', 'pure-sasl==0.6.2', 'py==1.8.0', 'pyarrow==3.0.0', 'pyasn1-modules==0.2.6', 'pyasn1==0.4.6', 'pyautogui==0.9.52', 'pybind11==2.5.0', 'pycparser==2.20', 'pygetwindow==0.0.9', 'pygments==2.6.1', 'pyjwt==2.1.0', 'pymsgbox==1.0.9', 'pyparsing==2.4.2', 'pyperclip==1.7.0', 'pyrect==0.1.4', 'pyrsistent==0.15.7', 'pyscreeze==0.1.27', 'python-dateutil==2.8.0', 'python3-xlib==0.15', 'pytorch-pretrained-bert==0.6.2', 'pytweening==1.0.3', 'pytz==2020.1', 'pyyaml==5.3.1', 'pyzmq==18.1.0', 'regex==2020.11.13', 'requests-oauthlib==1.2.0', 'requests==2.24.0', 'rsa==4.0', 's3transfer==0.2.1', 'sacremoses==0.0.39', 'scikit-learn==0.24.0', 'scipy==1.5.4', 'scons==3.1.2', 'send2trash==1.5.0', 'sentencepiece==0.1.91', 'seqeval==1.2.2', 'setuptools==50.3.2', 'six==1.15.0', 'smart-open==1.10.0', 'spacy==2.3.2', 'speechrecognition==3.8.1', 'srsly==1.0.2', 'symengine==0.8.1', 'tensorboard-plugin-wit==1.8.0', 'tensorboard==2.4.1', 'tensorboardx==2.0', 'tensorflow-estimator==2.4.0', 'tensorflow==2.4.1', 'termcolor==1.1.0', 'terminado==0.8.3', 'testpath==0.4.2', 'textblob==0.16.0', 'thinc==7.4.1', 'threadpoolctl==2.0.0', 'thrift==0.13.0', 'tokenizers==0.10.3', 'torch==1.7.1', 'tornado==6.0.3', 'tqdm==4.45.0', 'traitlets==5.0.4', 'transformers==4.6.1', 'twisted==15.4.0', 'typing-extensions==3.7.4', 'unidecode==1.1.1', 'urllib3==1.25.6', 'wasabi==0.6.0', 'wcwidth==0.1.7', 'webencodings==0.5.1', 'websocket-client==0.48.0', 'werkzeug==0.16.0', 'wheel==0.35.1', 'widgetsnbextension==3.5.1', 'word2number==1.1', 'wrapt==1.12.1', 'xxhash==2.0.2', 'zope.interface==5.1.0']\n"
     ]
    }
   ],
   "source": [
    "#checking the list of installed libraries\n",
    "installed_packages = pkg_resources.working_set\n",
    "installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version)\n",
    "   for i in installed_packages])\n",
    "print(installed_packages_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A and C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "filename : The training/ testing data filename can be passed\n",
    "Stopwords: Thie argument can be set to False/ True depending upon the users value\n",
    "\"\"\"\n",
    "def extracting_data(filename, Stopwords):\n",
    "    mylines = [] \n",
    "    with open (filename, 'rt') as myfile:  # Open filename for reading\n",
    "        for myline in myfile:              # For each line, read to a string,\n",
    "            mylines.append(myline.split('\\t',3))\n",
    "    extracted_list = [l[1:3] for l in mylines] # slicing the text and label from the text data\n",
    "    print(\"\\033[1m Please find the extracted list below:\")\n",
    "    print(extracted_list[0])\n",
    "    df = pd.DataFrame(extracted_list, columns = ['text', 'sentiment']) # Dataframe of sentiment and labels\n",
    "    df['text'] = df['text'].str.lower() # converting into lowercase\n",
    "    df['text'] = df['text'].str.replace(r'[^\\w\\s]+', '') # removing punctuations\n",
    "    pruned_df = df[~(df.sentiment == 'NEUTRAL')] # pruning the neutral sentiment\n",
    "    #The below lines help to remove the stopwords\n",
    "    if Stopwords:\n",
    "        removed_text = []\n",
    "        for text in pruned_df['text']:\n",
    "            words = [word for word in text.split() if word not in stopwords.words('english')]\n",
    "            new_text = \" \".join(words)\n",
    "            removed_text.append(new_text)\n",
    "        pruned_df['text'] = removed_text\n",
    "    return pruned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Please find the extracted list below:\n",
      "['This is definitely a must have if your state does not allow cell phone usage while driving.', 'POSITIVE']\n",
      "\u001b[1;32m The class distrbution for the training data is \n",
      " NEGATIVE    1282\n",
      "POSITIVE    1077\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-63c8eab223b2>:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is definitely a must have if your state d...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>its a great place and i highly recommend it</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i can tell you about having my phone and elect...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>their steaks are 100 recommended</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i was billed for thousands of dollars he said ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  this is definitely a must have if your state d...  POSITIVE\n",
       "1        its a great place and i highly recommend it  POSITIVE\n",
       "3  i can tell you about having my phone and elect...  NEGATIVE\n",
       "4                   their steaks are 100 recommended  POSITIVE\n",
       "5  i was billed for thousands of dollars he said ...  NEGATIVE"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data is passed to the extracting_data fucntion to get the dataframe\n",
    "training_data = extracting_data(\"PS1.1A_training_data.txt\", False)\n",
    "print(\"\\033[1;32m The class distrbution for the training data is \\n\",training_data['sentiment'].value_counts())\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Please find the extracted list below:\n",
      "['The reception through this headset is excellent.', 'POSITIVE']\n",
      "\u001b[1;32m The class distrbution for the testing data is \n",
      " NEGATIVE    1282\n",
      "POSITIVE    1077\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-63c8eab223b2>:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the reception through this headset is excellent</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hands down my favorite italian restaurant</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the bathrooms are clean and the place itself i...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you havent gone here go now</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>host staff were for lack of a better word bitches</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0    the reception through this headset is excellent  POSITIVE\n",
       "1          hands down my favorite italian restaurant  POSITIVE\n",
       "2  the bathrooms are clean and the place itself i...  POSITIVE\n",
       "3                     if you havent gone here go now  POSITIVE\n",
       "4  host staff were for lack of a better word bitches  NEGATIVE"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing data is passed to the extracting_data fucntion to get the dataframe\n",
    "testing_data = extracting_data(\"PS1.1A_test_data.txt\", False)\n",
    "print(\"\\033[1;32m The class distrbution for the testing data is \\n\",training_data['sentiment'].value_counts())\n",
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes_classifier(data, alpha):\n",
    "    #positive and negative sentiment data are splitted\n",
    "    positive_sentiment_data = data[data['sentiment'] == \"POSITIVE\"]\n",
    "    negative_sentiment_data = data[data['sentiment'] == \"NEGATIVE\"]\n",
    "    positive_count = len(positive_sentiment_data)\n",
    "    negative_count = len(negative_sentiment_data)\n",
    "    #calculating the prior probability of positive and negative sentiment\n",
    "    prior_positive_proba = np.log(positive_count / (positive_count+negative_count))\n",
    "    prior_negative_proba = np.log(negative_count / (positive_count+negative_count))\n",
    "    #The data is converted into list\n",
    "    combined_list = data.text.tolist()\n",
    "    positive_list = data[data.sentiment == \"POSITIVE\"].text.tolist()\n",
    "    negative_list = data[data.sentiment == \"NEGATIVE\"].text.tolist()\n",
    "    # From list of list it is converted to list\n",
    "    flat_combined_list = [item for item in combined_list]\n",
    "    flat_positive_list = [item for item in positive_list]\n",
    "    flat_negative_list = [item for item in negative_list]\n",
    "    #from list it is converted to string\n",
    "    combined_class = ' '.join(flat_combined_list)\n",
    "    positive_class = ' '.join(flat_positive_list)\n",
    "    negative_class = ' '.join(flat_negative_list)\n",
    "    #The strings are tokenized\n",
    "    combined_tokens = combined_class.split()\n",
    "    positive_tokens = positive_class.split()\n",
    "    negative_tokens = negative_class.split()\n",
    "    #Unique set of vocabulary is created \n",
    "    vocabulary = set(combined_tokens)\n",
    "    \n",
    "    positive_dictionary = {}\n",
    "    negative_dictionary = {}\n",
    "    # Dictionary is created with words as the keys and their count as the values\n",
    "    for word in positive_tokens:\n",
    "        positive_dictionary[word] = positive_tokens.count(word)\n",
    "    for word in negative_tokens:\n",
    "        negative_dictionary[word] = negative_tokens.count(word)\n",
    "    positive_liklihood = {}\n",
    "    negative_liklihood = {}\n",
    "    #The below loop is used to calculate the liklihoods of all the positive and negative sentiment for all the words in the vocabulary\n",
    "    for word in vocabulary:\n",
    "        pos_value = positive_dictionary.get(word)\n",
    "        neg_value = negative_dictionary.get(word)\n",
    "        if pos_value == None:\n",
    "            pos_value = 0\n",
    "        positive_liklihood[word] = np.log((pos_value+alpha)/(len(positive_tokens)+len(vocabulary)))\n",
    "        if neg_value == None:\n",
    "            neg_value = 0\n",
    "        negative_liklihood[word] = np.log((neg_value+alpha)/(len(negative_tokens)+len(vocabulary)))\n",
    "    \n",
    "    return positive_liklihood, negative_liklihood, prior_positive_proba, prior_negative_proba, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6575 4265\n"
     ]
    }
   ],
   "source": [
    "pos_lik, neg_lik, pos_prior, neg_prior, voc=train_naive_bayes_classifier(training_data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below function is used for test the naive bayes\n",
    "def test_naive_bayes(data,pos_lik, neg_lik, pos_prior, neg_prior, voc):\n",
    "    predicted = []\n",
    "    \n",
    "    #The below loop traverses every row on the test data\n",
    "    for doc in data['text']:\n",
    "        result = np.zeros((2,1))\n",
    "        text = nltk.word_tokenize(doc) #tokenized\n",
    "        result[0] = pos_prior \n",
    "        result[1] = neg_prior\n",
    "        # The below loop is used to calcualte the posterior probability of positive and negative senitment\n",
    "        for word in text:\n",
    "            if word in voc:\n",
    "                result[0] = pos_lik.get(word) + result[0]\n",
    "                result[1] = neg_lik.get(word) + result[1]\n",
    "        # The greatest posterior probabiltiy is appended to the predicted variable\n",
    "        if result[0] > result[1]:\n",
    "            predicted.append(\"POSITIVE\")\n",
    "        else:\n",
    "            predicted.append(\"NEGATIVE\")\n",
    "    #Calculating the metrics\n",
    "    print(classification_report(data['sentiment'], predicted, target_names=[\"POSITIVE\", \"NEGATIVE\"]))\n",
    "    cf_matrix = confusion_matrix(data['sentiment'], predicted)\n",
    "    print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    POSITIVE       0.75      0.80      0.78      1013\n",
      "    NEGATIVE       0.79      0.73      0.76      1002\n",
      "\n",
      "    accuracy                           0.77      2015\n",
      "   macro avg       0.77      0.77      0.77      2015\n",
      "weighted avg       0.77      0.77      0.77      2015\n",
      "\n",
      "[[813 200]\n",
      " [269 733]]\n"
     ]
    }
   ],
   "source": [
    "#Calling the test_naive_Bayes function\n",
    "test_naive_bayes(testing_data, pos_lik, neg_lik, pos_prior, neg_prior, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are **not removing** the stopwords from the text, the accuracy and F1 score are better compared to the text where the stopwords are **removed**. Why does this happen? When the stopwords have been removed the likelihood of the other words in the training data remains the same because the stopwords are independent of the other words in the training data. Now, during the testing, since there are no stopwords in the vocabulary the logarithmic addition will result in less posterior value compared to the posterior value when stopwords are present. This is happening because of stopwords appearing in the respective classes are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental drawback of the naive Bayes classification algorithm is that negation phrases like \"doesn't\" and \"haven't\" are ignored by the algorithm. To make our algorithm understand the difference between does and doesn't we can concatenate not with the doesn't (e.g.: don’t will be not_don’t), then our algorithm will take account of these words when we are calculating the likelihood, which will result in improving recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
