# -*- coding: utf-8 -*-
"""PS2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17LUt2LFtnnpCjmX2nYk6HZQtfxfKjWgt
"""

import nltk
import re
nltk.download('punkt')

class PART2:

  filename = ""
  raw_text =""
  tokens = []

  def __init__(self, filename):
    self.filename = filename
    print("Reading the file {}".format(filename.split(".")[0]))

  def cleaning(self):
    with open(self.filename) as file: # Opening the file
      print("Opening the file")
      line = file.readline() # Reading the file
      count = 1
      while(len(line) != 0):
        rawtokens = nltk.word_tokenize(line)
        ## throw out punctuation except dot and funny business
        for t in rawtokens:
          if "." in t:
            t = t.replace(".", "eos") # Adding eos instead of dot "." in our text files
          if t.isalpha():
            self.tokens.append(t)
        line = file.readline()
        count = count + 1

  def create_bigrams(self, tokens):
    bigrams = nltk.bigrams(tokens) # creating Bigrams
    return bigrams

  def ngramfreq_generator(self, tokens):
    ngramfdist = nltk.FreqDist(tokens) # Counting the frequency for n-gram list of words
    return ngramfdist
  
  def display_ngram_count(self, target, fdist):
    ## Here's some code that would print out all the bigrams
    ## and their frequencies. Uncomment to see what it does.
    if target == "bigram":
      print("BIGRAM COUNT")
      for k,v in fdist.items():
        print(k,v)
    elif target == "unigram":
      print("UNIGRAM COUNT")
      for k,v in fdist.items():
        print(k,v)

  def display_ngram_common(self, target, fdist):
    ## this tells you the top X most frequent ngrams 
    ## in the frequency distribution:
    print(fdist.most_common(5))


  def calculate_specific_count(self, string, fdist): #Calculating the count for specific words passed to the Func
    if string in fdist:
      return fdist[string]
    else:
      return 0

  #Creating a function for question 1 
  def Q2A_1(self, uni_frq, bi_frq):
    print("Q2A-1:")
    #Caclulating the unigram probability of the word "cook"
    total_words = len(self.tokens)
    cook_count = self.calculate_specific_count(("cook"), uni_frq)
    uni_prob = cook_count/total_words
    print("Unigram Probability of the word cook is {}".format(uni_prob))
    #Calculating the bigram probability of the word "your father"
    your_count = self.calculate_specific_count(("your"), uni_frq)
    bigram_count = self.calculate_specific_count(("your","father"), bi_frq)
    bi_prob = bigram_count/your_count
    print("Bigram Probability of the word \"your father\" is {}".format(bi_prob))

  #Creating a function for question 2
  def Q2A_2(self, string, uni_frq, bi_frq):

    print("Q2A-2:")
    str_bigram = self.create_bigrams(string.split(" ")) #Creating Bigrams
    
    prob_dictionary = {}
    for word in str_bigram:
      bigrm_cnt = self.calculate_specific_count(word,bi_frq)
      if bigrm_cnt !=0:
        print("Bigram count for the word \"{}\" is {}".format(word,bigrm_cnt))
        prob_dictionary[word] = bigrm_cnt/self.calculate_specific_count(word[0],uni_frq)
      elif self.calculate_specific_count(word[0],uni_frq) != 0 :
        print("BACKING OFF..!")
        print("Unigram count for the word \"{}\" is {}".format(word[0],self.calculate_specific_count(word[0],uni_frq)))
        prob_dictionary[word[0]] = self.calculate_specific_count(word[0],uni_frq)/len(self.tokens)

    answer = 1
    for i in prob_dictionary:
      answer = answer*prob_dictionary[i]
    print("The sentence probability for the following sentence \"{}\" is {}".format(string,answer))

  #Creating a function for question 3
  def Q3A_3(self, uni_frq, bi_frq):
    print("Q3A_3:")
    string = ""
    First_word = ""
    max_uni = 1
    i=0
    for k,v in uni_frq.items():
      if re.match("[A-Z][a-zA-Z]*", k) and v>max_uni: # Condition to check for capitalized word and check the max count
        #print(k,v)
        First_word = k
        max_uni = v
    string+=First_word
    while i < 5:
      second_word = ""
      max_bi = 0
      print(string.split(" "))
      for k, v in bi_frq.items():
        # Condition to check the lowercased word, to check not eos and checking the second word in bigram with the tuple dictionary to continue the sequence 
        if string.split(" ")[i] == k[0] and v>max_bi and k[0]!= "eos" and k[1]!="eos" and k[1].islower() == True:
          #print(k,v)
          max_bi = v
          second_word = k[1]
      string+= " " + second_word
      i+=1
    print("The most likely 6 word sentence is : \"{}\" ".format(string))

obj = PART2("pg1112.txt") # Reading file 1
obj.cleaning() #Tokenizing and replacing dot with "eos"
bigrams1 = obj.create_bigrams(obj.tokens) #Creating Bigrams
uni_frq1 = obj.ngramfreq_generator(obj.tokens) #Calculating Unigram count
bi_frq1 = obj.ngramfreq_generator(bigrams1) #Calculating Bigram count
obj.Q2A_1(uni_frq1, bi_frq1) # Calling the function to do Question 1
obj.Q2A_2("eos You should have been a fellow eos", uni_frq1, bi_frq1) # Calling the function to do Question 2
obj.Q3A_3(uni_frq1, bi_frq1) # Calling the function to do Question 3

obj2 = PART2("pg31547.txt") # Reading file 2
obj2.cleaning() #Tokenizing and replacing dot with "eos"
bigrams2 = obj2.create_bigrams(obj2.tokens) #Creating Bigrams
uni_frq2 = obj2.ngramfreq_generator(obj2.tokens) #Calculating Unigram count
bi_frq2 = obj2.ngramfreq_generator(bigrams2) #Calculating Bigram count
obj2.Q2A_1(uni_frq2, bi_frq2) # Calling the function to do Question 1
obj2.Q2A_2("eos You should have been a fellow eos", uni_frq2, bi_frq2) # Calling the function to do Question 2
obj2.Q3A_3(uni_frq2, bi_frq2) # Calling the function to do Question 3